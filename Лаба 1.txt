Лабораторная работа 1. Language Modelling

В самом начале работы должно быть краткое описание сделанного и результатов. В конце будут более подробные выводы.

0. Выберите два текста (или набора текстов), с которым будете работать. Это могут быть художественные произведения, публицистика, научпоп и т.д. Пусть один корпус будет тренировочным, а второй -- тестовым. Лучше взять текст (или несколько, например, для серии книг или статей одного издания) побольше, чтобы у модели было больше данных для обучения.

1. Попробуйте разные способы токенизации: regex, nltk (там несколько токенизаторов), другие библиотеки. Учитывать ли пунктуацию? Делить ли слова с дефисами и апострофами? Сохранять ли заглавные буквы? Сделать ли специальные токены для начала и конца предложения?
Мы будем обучать языковую модель, и такие вещи могут на неё повлиять. По этой же причине удобнее каждый вариант обернуть в функцию, чтобы можно было быстро попробовать разные варианты.

2. Обучите языковую модель на n-граммах на вашем тренировочном корпусе. Попробуйте разное значение n.
	
3. Реализуйте сглаживание. Попробуйте модели без него, с add-1 (Laplace), линейной интерполяцией, Katz backoff и др. способами. Для незнакомого токена также проверяйте, не является ли он вариацией знакомого с дистанцией редактирования=1 (мы вычисляли её на семинаре, но можно воспользоваться библиотеками) и выбирайте из подходящих наиболее вероятных вариантов. При выборе можно учесть не только n-граммную вероятность, но и вероятность такой опечатки по близости на клавиатуре.

4. Посчитайте перплексии модели с разными настройками на тестовом корпусе. Какая архитектура лучше сработала для вашего текста?

5. Научите вашу лучшую модель генерировать текст. Несколько способов (есть и другие): 
	- выбор следующего самого вероятного токена;
	- случайный выбор одного из нескольких самых вероятных токенов:
		- с одинаковой вероятностью,
		- с перераспределением вероятности в зависимости от индекса в топе;
	- beam search: выбирается n самых вероятных токенов, для каждого из них выбирается ещё n, и так несколько раз, после чего выбирается последовательность с наибольшей общей вероятностью. 

Попробуйте разные методы генерации в случаях, когда токеном, с которого начинается генерация, является:
	1) токен начала предложения;
	2) случайный токен;
	3) токен не из словаря (например, слово с опечаткой).

6. Поделитесь мыслями по поводу получившегося. Какие комбинации были удачными? Почему? Как на это повлияла специфика жанра или текста? Что можно было бы ещё сделать? Что могло бы пойти по-другому? И т.д.

Дедлайн: 2 недели. За каждый день просрочки отнимается 0,5 балла, но не больше половины от общей оценки (поэтому сдать лабу поздно всегда выгоднее, чем не сдать вообще!). 
Система оценивания: за каждое задание вы будете получать первичные баллы, которые потом я сконвертирую в оценки до 8 в зависимости от максимального и минимального балла в группе. За каждый опробованный способ и вариацию -- +1 балл.

Совет: не дебажьте на всех данных, это может быть долго! Напишите код, проверяя шаги на небольшом объёме текста, а потом, когда всё отдебажите, запустите на всём корпусе.